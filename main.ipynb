{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63375715-1e2e-4cda-98bb-ae371a3c99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import your modules\n",
    "from data.data_generator import main as generate_data, load_and_stack_datasets\n",
    "from data.data_loader import load_data, MyDataset\n",
    "from models.model import CustomNN\n",
    "from utils.training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26a6a2e6-0a8b-4d87-a659-6e3b4345aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "{'input_size': 1851, 'output_size': 1800, 'hidden_layers': 4, 'neurons_per_layer': [3072], 'dropout_prob': 0.0, 'use_batch_norm': False, 'activation': 'leaky_relu', 'learning_rate': 0.0001, 'epochs': 51, 'batch_size': 1024, 'model_save_prefix': './'}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dictionary\n",
    "config = {\n",
    "    'input_size': 1851,\n",
    "    'output_size': 1800,\n",
    "    'hidden_layers': 4,\n",
    "    'neurons_per_layer': [3072],  # Base neuron count; will be repeated hidden_layers times.\n",
    "    'dropout_prob': 0.0,\n",
    "    'use_batch_norm': False,\n",
    "    'activation': 'leaky_relu',\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 51,\n",
    "    'batch_size': 1024,\n",
    "    'model_save_prefix': './'\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4db20b44-e80b-4e1d-a477-c3199b179579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already generated. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "# List of data files (updated names)\n",
    "data_files = [\n",
    "    'data/xtrain_nl2.pt',\n",
    "    'data/ytrain_nl2.pt',\n",
    "    'data/x_valid_nl2.pt',\n",
    "    'data/y_valid_nl2.pt',\n",
    "    'data/norm_data_nl2.nc'\n",
    "]\n",
    "\n",
    "# Check if data files exist; if not, generate them.\n",
    "if not all(os.path.exists(f) for f in data_files):\n",
    "    print(\"Generating data...\")\n",
    "    generate_data()  # This calls your data_generator.py main() function\n",
    "    print(\"Data generation complete.\")\n",
    "else:\n",
    "    print(\"Data already generated. Skipping generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e340a6cf-9a97-4da0-a764-0f6e87b8e69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Load the data using your data loader module\n",
    "x, y, x_valid, y_valid, norm_data = load_data()\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MyDataset(x, y)\n",
    "valid_dataset = MyDataset(x_valid, y_valid)\n",
    "\n",
    "# Create DataLoaders using the batch size from config\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=False, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=False, drop_last=True)\n",
    "\n",
    "print(\"Data loaded and DataLoaders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2e61a33-5c33-4360-9a81-b7c96d65d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1           [-1, 1024, 3072]       5,689,344\n",
      "         LeakyReLU-2           [-1, 1024, 3072]               0\n",
      "            Linear-3           [-1, 1024, 3072]       9,440,256\n",
      "         LeakyReLU-4           [-1, 1024, 3072]               0\n",
      "            Linear-5           [-1, 1024, 3072]       9,440,256\n",
      "         LeakyReLU-6           [-1, 1024, 3072]               0\n",
      "            Linear-7           [-1, 1024, 3072]       9,440,256\n",
      "         LeakyReLU-8           [-1, 1024, 3072]               0\n",
      "            Linear-9           [-1, 1024, 3072]       9,440,256\n",
      "        LeakyReLU-10           [-1, 1024, 3072]               0\n",
      "           Linear-11           [-1, 1024, 1800]       5,531,400\n",
      "================================================================\n",
      "Total params: 48,981,768\n",
      "Trainable params: 48,981,768\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 7.23\n",
      "Forward/backward pass size (MB): 254.06\n",
      "Params size (MB): 186.85\n",
      "Estimated Total Size (MB): 448.14\n",
      "----------------------------------------------------------------\n",
      "Model built on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Repeat base neuron count by number of hidden layers\n",
    "neurons_list = config['neurons_per_layer'] * config['hidden_layers']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the model using your CustomNN class\n",
    "model = CustomNN(\n",
    "    config['input_size'],\n",
    "    config['output_size'],\n",
    "    config['hidden_layers'],\n",
    "    neurons_list,\n",
    "    config['dropout_prob'],\n",
    "    config['use_batch_norm'],\n",
    "    config['activation']\n",
    ").to(device)\n",
    "\n",
    "# Display model summary\n",
    "summary(model, (config['batch_size'], config['input_size']))\n",
    "print(\"Model built on device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1288747-f18f-40f6-8b20-81d0b53c565a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# Create a normalization tensor from norm_data\n",
    "fac = 1e6\n",
    "a1 = (fac * norm_data['VNL_std'] + 0).transpose().values\n",
    "norm_tensor = torch.FloatTensor(a1).to(device)\n",
    "\n",
    "def criterion(output, target):\n",
    "    return torch.mean(torch.abs(norm_tensor * (target - output)))\n",
    "\n",
    "print(\"Custom loss function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "007b601d-8ab6-488d-882f-d073a2650f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  1 / 51\n",
      "Model saved at epoch 5\n",
      "Epochs:  10 / 51\n",
      "Epochs:  20 / 51\n",
      "Epochs:  30 / 51\n",
      "Epochs:  40 / 51\n",
      "Epochs:  50 / 51\n",
      "Training completed in 2.94 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create dynamic model save name using config parameters:\n",
    "model_save_name = f\"{config['model_save_prefix']}L{config['hidden_layers']}_{neurons_list[0]}_\"\n",
    "model_save_name\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], amsgrad=True)\n",
    "\n",
    "# Train the model and record training results\n",
    "start_time = time.time()\n",
    "training_results = train(\n",
    "    model, criterion, train_dataloader, valid_dataloader,\n",
    "    optimizer, config['epochs'], model_save_name, device, save_epochs=[5]\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(\"Training completed in {:.2f} seconds\".format(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5e919a3-a7b8-4221-8399-afaffd8e9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = f\"{config['model_save_prefix']}L{config['hidden_layers']}_{neurons_list[0]}_{config['epochs']}epoch\"\n",
    " \n",
    "# Save training logs\n",
    "np.savetxt(model_save_name + '_tr.txt', np.array(training_results['train_loss']))\n",
    "np.savetxt(model_save_name + '_vd.txt', np.array(training_results['valid_loss']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo_env",
   "language": "python",
   "name": "pangeo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
